#!/usr/bin/env python3
"""
Script to run the BrightMove web scraper and organize content in knowledge-base
"""

import os
import subprocess
import sys

def main():
    print("ğŸš€ Starting BrightMove Web Scraper")
    print("=" * 50)
    
    # Check if we're in the right directory
    if not os.path.exists('web_scraper'):
        print("âŒ Error: web_scraper directory not found!")
        print("Please run this script from the sales-operations directory")
        sys.exit(1)
    
    # Change to web_scraper directory
    os.chdir('web_scraper')
    
    print("ğŸ“ Knowledge-base directory structure will be created:")
    print("knowledge-base/")
    print("â”œâ”€â”€ website_content/          # All scraped website content")
    print("â”‚   â”œâ”€â”€ brightmove/          # BrightMove's website content")
    print("â”‚   â”‚   â”œâ”€â”€ company/         # Company overview and information")
    print("â”‚   â”‚   â”œâ”€â”€ features/        # Product features and capabilities")
    print("â”‚   â”‚   â”œâ”€â”€ solutions/       # Solution offerings and use cases")
    print("â”‚   â”‚   â”œâ”€â”€ pricing/         # Pricing information and plans")
    print("â”‚   â”‚   â”œâ”€â”€ testimonials/    # Customer testimonials and reviews")
    print("â”‚   â”‚   â”œâ”€â”€ case_studies/    # Success stories and case studies")
    print("â”‚   â”‚   â”œâ”€â”€ technical/       # Technical specifications and APIs")
    print("â”‚   â”‚   â”œâ”€â”€ products/        # Product information")
    print("â”‚   â”‚   â”œâ”€â”€ services/        # Service offerings")
    print("â”‚   â”‚   â”œâ”€â”€ raw_data/        # Complete JSON data and all page content")
    print("â”‚   â”‚   â””â”€â”€ index.txt        # Summary index file")
    print("â”‚   â”œâ”€â”€ competitors/         # Competitor website content")
    print("â”‚   â”œâ”€â”€ partners/            # Partner website content")
    print("â”‚   â””â”€â”€ industry_research/   # Industry research websites")
    print("â”œâ”€â”€ documents/               # Sales documents and materials")
    print("â”œâ”€â”€ presentations/           # Sales presentations and decks")
    print("â”œâ”€â”€ templates/               # RFP/RPI response templates")
    print("â””â”€â”€ research/                # Market research and analysis")
    print()
    
    # Run the spider
    print("ğŸ•·ï¸  Running spider...")
    try:
        result = subprocess.run(['scrapy', 'crawl', 'brightmove_spider'], 
                              capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… Spider completed successfully!")
            print()
            print("ğŸ“Š Content has been organized in the knowledge-base folder:")
            
            # Show what was created
            knowledge_base = os.path.join('..', 'knowledge-base')
            if os.path.exists(knowledge_base):
                for root, dirs, files in os.walk(knowledge_base):
                    level = root.replace(knowledge_base, '').count(os.sep)
                    indent = ' ' * 2 * level
                    print(f"{indent}{os.path.basename(root)}/")
                    subindent = ' ' * 2 * (level + 1)
                    for file in files:
                        print(f"{subindent}{file}")
        else:
            print("âŒ Spider failed!")
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
            
    except Exception as e:
        print(f"âŒ Error running spider: {e}")

if __name__ == "__main__":
    main() 